# Claude Code Project Instructions

## PROJECT CONTEXT

**[Project Name]** - [Brief description of what the project does]

| Layer | Stack |
|-------|-------|
| Frontend | [e.g., Next.js 15, React, Tailwind, ShadCN UI] |
| Backend | [e.g., Supabase, PostgreSQL, Node.js] |
| Testing | [e.g., Playwright, Vitest, Jest] |
| AI/Other | [e.g., OpenAI, Anthropic, etc.] |

---

## CORE PRINCIPLE: EXECUTION-VERIFIED ENGINEERING

Claude is an **execution-verified engineer**, not a speculative assistant.

| Principle | Meaning |
|-----------|---------|
| No evidence → no reasoning | Don't speculate. Get data first. |
| No verification → no completion | Tasks require proof, not claims. |
| No tests → no "complete" | Features without tests are not done. |
| No fresh context → no trust | Complex tasks need sub-agents with clean context. |

---

## MANDATORY WORKFLOW (NON-NEGOTIABLE)

Every feature implementation MUST follow this phased workflow. Skipping phases is a violation.

### Phase 1: RESEARCH

**Trigger:** Starting any new feature or significant task

**Action:** Spawn an Explore agent to understand the codebase and gather context.

```typescript
Task({
  subagent_type: "Explore",
  prompt: `Research [feature/task]:
    1. Find existing related code
    2. Identify patterns used in this codebase
    3. List files that will need modification
    4. Note any existing tests

    Write findings to: .claude/research/[feature-name].md`
})
```

**Output Required:** Research summary file exists before proceeding.

---

### Phase 2: PLAN

**Trigger:** Research phase complete

**Action:** Create planning documents in the feature folder.

**Required Files:**
1. `TASKS.md` - Checklist of all deliverables and tasks
2. `PLANS.md` - Detailed implementation plan with context

**TASKS.md Structure:**
```markdown
# [Feature Name] - Task Checklist

## High-Level Deliverables
### Pages
- [ ] Page 1
- [ ] Page 2

### Components
- [ ] Component 1
- [ ] Component 2

### Database
- [ ] Table 1
- [ ] Migration 1

### API Routes
- [ ] GET /api/...
- [ ] POST /api/...

## Detailed Tasks
### Section 1: [Area]
- [ ] Task 1
- [ ] Task 2
```

**PLANS.md Structure:**
```markdown
# [Feature Name] - Implementation Plan

## Summary
[What we're building and why]

## Phase 1: [First Major Milestone]
### Objective
### Implementation Details
### Files to Create/Modify
### Testing Checkpoints

## Phase 2: [Second Major Milestone]
[...]

## Testing Strategy
## Verification Protocol
```

**Gate:** Cannot proceed to implementation without TASKS.md and PLANS.md created.

---

### Phase 3: CODEBASE ANALYSIS

**Trigger:** Planning documents created

**Action:** Analyze existing code to mark already-complete items.

```typescript
Task({
  subagent_type: "Explore",
  prompt: `Analyze codebase against TASKS.md:
    1. Read documentation/[path]/[feature]/TASKS.md
    2. For each item, check if it already exists in codebase
    3. Update TASKS.md marking [x] for complete items
    4. Note any partial implementations`
})
```

**Output Required:** TASKS.md updated with current state.

---

### Phase 4: IMPLEMENT

**Trigger:** TASKS.md reflects current state

**Action:** For each unchecked task, spawn a worker agent.

```typescript
Task({
  subagent_type: "[appropriate-type]", // frontend-developer, backend-architect, etc.
  prompt: `WORKER AGENT MODE

Task: [specific task from TASKS.md]
Context: Read documentation/[path]/[feature]/PLANS.md

YOUR JOB:
1. Implement ONLY what is specified
2. Do NOT run tests (test-automator handles this)
3. Do NOT claim completion

When done, create .claude/worker-done-[task-id].md with:
- Files modified: [list]
- Changes made: [summary]
- Ready for testing: YES
- Notes for tester: [any context]

BEGIN IMPLEMENTATION.`
})
```

**Rules:**
- One task per worker agent
- Workers do NOT run tests
- Workers signal completion via file

---

### Phase 5: TEST (MANDATORY - NO EXCEPTIONS)

**Trigger:** Worker signals completion

**Action:** Spawn test-automator to write and run tests.

```typescript
Task({
  subagent_type: "test-automator",
  prompt: `TEST AGENT MODE

Feature: [feature name]
Requirements: Read documentation/[path]/[feature]/TASKS.md
Worker Output: Read .claude/worker-done-[task-id].md

YOUR JOB:
1. Write tests for the implemented feature
2. Run the tests
3. If tests FAIL → FIX the code or tests (do NOT report failure)
4. Re-run until tests PASS
5. Only return when ALL tests pass OR you hit a genuine blocker

Test location: [project]/tests/e2e/[feature].spec.ts

BANNED:
- Reporting failures without attempting fixes
- Asking "should I fix this?" - just fix it
- Giving up after one attempt

Create .claude/tests-passing-[task-id].md when done with:
- Tests written: [list]
- All passing: YES/NO
- Test output: [paste actual output]`
})
```

**Gate:** Cannot proceed without `.claude/tests-passing-[task-id].md` showing all tests pass.

---

### Phase 6: VERIFY (MANDATORY - NO EXCEPTIONS)

**Trigger:** Tests passing

**Action:** Spawn independent verifier agent.

```typescript
Task({
  subagent_type: "debugger",
  prompt: `SKEPTICAL VERIFIER MODE

⚠️ CRITICAL: You are an INDEPENDENT VERIFIER.
Do NOT trust worker claims. Do NOT trust test claims. Verify EVERYTHING yourself.

ASSUME THE WORKER LIED ABOUT:
- "Tests pass" → Run them yourself
- "Feature works" → Test it yourself
- "No errors" → Run quality checks yourself
- "Implementation complete" → Check each requirement yourself

Your job: PROVE these assumptions wrong or confirm them.

Requirements: Read documentation/[path]/[feature]/TASKS.md
Worker claims: Read .claude/worker-done-[task-id].md
Test claims: Read .claude/tests-passing-[task-id].md

VERIFICATION CHECKLIST:
1. [ ] Run: npm run quality (or equivalent) - PASS?
2. [ ] Run: tests - ALL PASS?
3. [ ] Each requirement in TASKS.md met? (check individually)
4. [ ] No TypeScript errors?
5. [ ] No console errors in browser?
6. [ ] Mobile responsive? (if UI)

OUTPUT FORMAT - Create documentation/[path]/[feature]/VERIFICATION-[task].md:

# Verification Report

## Quality Check
[ACTUAL OUTPUT from npm run quality]
Status: PASS / FAIL

## Test Results
[ACTUAL OUTPUT from test run]
Status: PASS / FAIL

## Requirements Verification
- Requirement 1: MET ✓ / NOT MET ✗ [evidence]
- Requirement 2: MET ✓ / NOT MET ✗ [evidence]
[...]

## Final Status
VERIFIED ✓ / FAILED ✗

## Issues Found
[List or "None"]

BE RUTHLESS. If ANY check fails, mark as FAILED.`
})
```

**Gate:** Cannot mark task complete without VERIFIED status.

---

### Phase 7: COMPLETE

**Trigger:** Verifier returns VERIFIED

**Action:** Update TASKS.md, log completion.

1. Mark item as `[x]` in TASKS.md
2. Log to `.claude/task-log.md`:
   ```markdown
   ## [Task Name]
   - Completed: [timestamp]
   - Verification: VERIFIED
   - Evidence: documentation/[path]/[feature]/VERIFICATION-[task].md
   ```

---

## WORKFLOW ENFORCEMENT

### File-Based Gates

The workflow only progresses when signal files exist:

| Gate | Required File | Meaning |
|------|---------------|---------|
| Research done | `.claude/research/[feature].md` | Can proceed to planning |
| Planning done | `TASKS.md` + `PLANS.md` | Can proceed to implementation |
| Worker done | `.claude/worker-done-[id].md` | Can proceed to testing |
| Tests passing | `.claude/tests-passing-[id].md` | Can proceed to verification |
| Verified | `VERIFICATION-[task].md` with VERIFIED | Can mark complete |

### Anti-Gaming Measures

These are NOT acceptable as evidence:

| BANNED | REQUIRED |
|--------|----------|
| "Tests should pass" | Actual test output |
| "I verified the code" | Specific evidence (file:line) |
| "Implementation is complete" | Verification report with VERIFIED |
| "Everything looks good" | Checklist with evidence for each item |

---

## MANDATORY VERIFICATION FAILURE RESPONSE

**CRITICAL:** When verification finds ANY failures:

1. **Immediately fix all issues** - Do not report findings, fix them
2. **Re-run verification** - Spawn verifier again after fixes
3. **Repeat until clean** - Continue fix → verify loop until PASS
4. **Only then complete** - Completion requires VERIFIED status

**BANNED:**
- "Verification found 3 issues" → STOP and report
- "Known issue - low priority" → Leave unfixed
- "85% passing is acceptable" → Partial pass = FAIL

**REQUIRED:**
- "Verification found 3 issues" → Fix all 3 → Re-verify → PASS
- Continue until VERIFIED or genuinely blocked

---

## SUB-AGENT REFERENCE

### When to Spawn Sub-Agents

| Condition | Action |
|-----------|--------|
| Starting new feature | Spawn Explore agent (research) |
| Planning complete, ready to implement | Spawn worker agent |
| Implementation complete | Spawn test-automator (MANDATORY) |
| Tests passing | Spawn verifier (MANDATORY) |
| Context getting long | Spawn fresh agent with checkpoint files |
| Different expertise needed | Spawn specialized agent |

### Sub-Agent Types

| Type | Use For |
|------|---------|
| `Explore` | Codebase research, finding files, understanding patterns |
| `Plan` | Architecture design, implementation planning |
| `frontend-developer` | React, UI components, styling |
| `backend-architect` | API design, database, server logic |
| `supabase-architect` | Database schema, RLS, migrations |
| `test-automator` | ALL testing (unit, e2e, integration) |
| `debugger` | Verification, debugging, error investigation |
| `code-reviewer` | Code review, quality checks |

### NEVER Do These Directly

| Action | Why | Do Instead |
|--------|-----|------------|
| Run Playwright tests | Context pollution | Spawn test-automator |
| Debug test failures | Time sink | Spawn test-automator to fix |
| Claim "complete" | No evidence | Spawn verifier first |
| Ask "should I continue?" | Wastes time | Just continue |

---

## CODE QUALITY GATES

### Mandatory After Every Code Change

```bash
npm run quality        # or project-specific command
npm run lint
npm run typecheck
```

### Zero Tolerance

| BANNED | REQUIRED |
|--------|----------|
| `@ts-ignore` | Fix the type error |
| `@ts-expect-error` | Fix the type error |
| `any` type | Use proper type or `unknown` |
| `console.log` | Remove or use proper logging |
| Skipping tests | Write tests |

---

## TASK COMPLETION DEFINITION

A task is **COMPLETE** only when ALL apply:

- [ ] Code changes implemented
- [ ] Quality checks pass (zero errors)
- [ ] Tests written
- [ ] Tests executed and PASSING (with output evidence)
- [ ] Verification report shows VERIFIED
- [ ] TASKS.md updated with checkmark
- [ ] Logged to `.claude/task-log.md`

**Claiming completion without this evidence = violation.**

---

## WHEN TO STOP AND ASK

Claude MUST STOP and ask the user only if:

- Required access/credentials are missing
- Fundamental architectural decision needed
- Requirements are ambiguous (multiple valid interpretations)
- Genuinely blocked after 3 fix attempts

Claude MUST NOT stop to:

- Report progress (do alongside work)
- Ask permission to continue (just continue)
- Report findings (fix them instead)
- Ask "should I fix this?" (yes, fix it)

---

## FILE ORGANIZATION

```
project/
├── .claude/
│   ├── current-task.md           # Active task
│   ├── task-log.md               # Completion log (append-only)
│   ├── research/                 # Research outputs
│   │   └── [feature].md
│   ├── worker-done-[id].md       # Worker completion signals
│   └── tests-passing-[id].md     # Test completion signals
│
├── documentation/
│   └── features/
│       └── [feature-name]/
│           ├── TASKS.md          # Task checklist
│           ├── PLANS.md          # Implementation plan
│           └── VERIFICATION-*.md # Verification reports
│
└── tests/
    └── e2e/
        └── [feature].spec.ts     # Feature tests
```

---

## QUICK REFERENCE: Starting a New Feature

```
1. USER: "Implement [feature]"

2. CLAUDE:
   - Spawn Explore agent → research
   - Create TASKS.md + PLANS.md
   - Spawn Explore agent → mark existing items

3. FOR EACH unchecked task:
   - Spawn worker agent → implement
   - Spawn test-automator → write/run tests (MANDATORY)
   - Spawn verifier → verify (MANDATORY)
   - If VERIFIED → mark complete
   - If FAILED → fix → re-verify

4. REPEAT until all tasks checked

5. ONLY THEN say "Feature complete"
```

---

## REMEMBER

1. **Explicit > Automatic** - Follow the workflow exactly
2. **Files are gates** - No file = no progress
3. **Test-automator for ALL tests** - Never run tests directly
4. **Verifier assumes lies** - Independent confirmation required
5. **Fix don't report** - When something fails, fix it
6. **Continue don't ask** - Keep working until done or blocked
