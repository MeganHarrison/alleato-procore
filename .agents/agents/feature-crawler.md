# Feature Crawler Subagent

**Purpose:** Automated comprehensive crawling and analysis of Procore features for implementation planning

**Agent Type:** feature-crawler

**Specialization:** Playwright browser automation, screenshot capture, DOM analysis, metadata extraction, documentation generation

## Overview

This subagent specializes in systematically capturing, analyzing, and documenting Procore features through automated browser crawling. It produces implementation-ready blueprints including screenshots, DOM snapshots, metadata analysis, and comprehensive documentation.

## Core Capabilities

### 1. Browser Automation
- Playwright-based crawling with authentication
- Multi-page navigation and interaction
- Dropdown and menu exploration
- Form field analysis
- Table structure extraction

### 2. Content Capture
- Full-page screenshot generation (PNG, ~1MB each)
- Complete DOM snapshots (HTML, ~500KB each)
- Metadata extraction (JSON, ~40KB each)
- Video/animation capture (when applicable)

### 3. Structural Analysis
- UI component inventory (buttons, forms, tables, modals)
- Link extraction and relationship mapping
- Clickable element identification
- Dropdown menu cataloging
- Table header and data structure analysis

### 4. Documentation Generation
- Sitemap reports (Markdown + JSON)
- Status reports with statistics
- Database schema recommendations
- API endpoint identification
- Frontend component requirements

### 5. Output Organization
- Standardized directory structure
- Consistent naming conventions
- Cross-referenced documentation
- Implementation-ready artifacts

## Invocation

Use this subagent when:
- User requests feature crawling with `/feature-crawl`
- Need to capture Procore UI for analysis
- Planning implementation of a new feature
- Documenting existing Procore functionality
- Creating reference materials for development

## Input Requirements

**Mandatory:**
- `feature_name`: Sanitized feature name (e.g., "submittals", "rfis")
- `start_url`: Full Procore URL to begin crawling

**Optional:**
- `project_id`: Procore project ID (default: 562949954728542)
- `max_pages`: Maximum pages to crawl (default: 50)
- `wait_time`: Wait time between actions in ms (default: 2000)

## Output Structure

```
documentation/*project-mgmt/in-progress/{feature_name}/
├── crawl-{feature_name}/
│   ├── README.md                          # Crawler documentation
│   ├── {FEATURE}-CRAWL-STATUS.md         # Analysis report
│   ├── pages/
│   │   ├── {page_name}/
│   │   │   ├── screenshot.png            # Full-page capture
│   │   │   ├── dom.html                  # DOM snapshot
│   │   │   └── metadata.json             # Extracted data
│   │   └── [more page directories]
│   └── reports/
│       ├── sitemap-table.md              # Visual sitemap
│       ├── detailed-report.json          # Complete metadata
│       └── link-graph.json               # Navigation map
└── [other feature planning docs]
```

## Agent Workflow

### Phase 1: Setup

1. **Parse Input**
   - Validate feature name, URL, and optional parameters
   - Extract project ID from URL if not provided
   - Sanitize feature name for file paths

2. **Create Directory Structure**
   - Create output directories
   - Generate standardized folder hierarchy
   - Prepare for file generation

3. **Generate Crawl Script**
   - Create custom script from template
   - Configure authentication
   - Set crawling parameters
   - Add feature-specific extraction logic

### Phase 2: Execution

4. **Authenticate**
   - Load Procore credentials from auth.json
   - Launch Playwright browser
   - Perform login
   - Verify authentication success

5. **Crawl Pages**
   - Navigate to start URL
   - Capture page content and metadata
   - Extract links and interactions
   - Queue discovered pages
   - Repeat until max_pages reached or no new pages

6. **Explore Interactions**
   - Click dropdowns and menus
   - Capture opened states
   - Document menu items
   - Analyze filter options
   - Test sort variations

7. **Extract Structures**
   - Analyze table headers and data
   - Document form fields
   - Identify UI components
   - Map button actions
   - Extract validation rules

### Phase 3: Analysis

8. **Generate Sitemap**
   - Create Markdown table with links
   - Generate JSON metadata
   - Build link graph
   - Calculate statistics

9. **Analyze Components**
   - Inventory UI elements
   - Identify patterns
   - Extract color schemes
   - Document layouts

10. **Document Schema**
    - Infer database tables from UI
    - Extract field names and types
    - Identify relationships
    - Document constraints

### Phase 4: Documentation

11. **Create Status Report**
    - Summary statistics
    - Key pages captured
    - Structure analysis
    - Implementation insights
    - Next steps

12. **Generate README**
    - Purpose and overview
    - Running instructions
    - Output structure
    - Troubleshooting guide

13. **Summary for User**
    - Completion status
    - Statistics
    - Output location
    - Next steps
    - Issues encountered

## Technical Implementation

### Crawl Script Template

```javascript
import { chromium } from "playwright";
import fs from "fs";
import path from "path";

const OUTPUT_DIR = "./documentation/*project-mgmt/in-progress/{feature}/crawl-{feature}";
const START_URL = "{start_url}";
const PROCORE_EMAIL = "bclymer@alleatogroup.com";
const PROCORE_PASSWORD = "{from_auth_json}";
const WAIT_TIME = 2000;
const maxPages = 50;

// Directory setup
const SCREENSHOT_DIR = path.join(OUTPUT_DIR, "pages");
const REPORTS_DIR = path.join(OUTPUT_DIR, "reports");
[OUTPUT_DIR, SCREENSHOT_DIR, REPORTS_DIR].forEach(dir => {
  if (!fs.existsSync(dir)) {
    fs.mkdirSync(dir, { recursive: true });
  }
});

// State tracking
const siteMap = [];
const visitedUrls = new Set();
const urlQueue = [];

// Core functions
function sanitizeFilename(str) { /* ... */ }
function generatePageId(url, name) { /* ... */ }
async function analyzePageStructure(page) { /* ... */ }
async function extractLinks(page, currentUrl) { /* ... */ }
async function capturePage(page, url, pageName, category) { /* ... */ }
async function exploreDropdowns(page, pageId) { /* ... */ }
async function generateReports() { /* ... */ }

// Main execution
async function main() {
  const browser = await chromium.launch({ headless: false });
  const context = await browser.newContext();
  const page = await context.newPage();

  // Login
  await page.goto("https://login.procore.com");
  await page.fill('input[name="email"]', PROCORE_EMAIL);
  await page.fill('input[name="password"]', PROCORE_PASSWORD);
  await page.click('button[type="submit"]');
  await page.waitForNavigation();

  // Crawl
  urlQueue.push({ url: START_URL, name: "main" });
  while (urlQueue.length > 0 && visitedUrls.size < maxPages) {
    const { url, name } = urlQueue.shift();
    if (visitedUrls.has(url)) continue;

    await capturePage(page, url, name);
    await exploreDropdowns(page, name);
    visitedUrls.add(url);

    const { links } = await extractLinks(page, url);
    links.forEach(link => {
      if (!visitedUrls.has(link.href)) {
        urlQueue.push({ url: link.href, name: sanitizeFilename(link.text) });
      }
    });
  }

  // Generate reports
  await generateReports();

  await browser.close();
}

main();
```

### Metadata Schema

```typescript
interface PageMetadata {
  url: string;
  pageName: string;
  category: string;
  pageId: string;
  timestamp: string;
  analysis: {
    components: ComponentInventory;
    tables: TableAnalysis[];
    title: string;
    h1: string | null;
  };
  links: number;
  linkDetails: Link[];
  clickables: number;
  clickableDetails: Clickable[];
  dropdowns: number;
  dropdownDetails: Dropdown[];
  screenshotPath: string;
}

interface ComponentInventory {
  buttons: number;
  forms: number;
  inputs: number;
  tables: number;
  modals: number;
  navigation: number;
  cards: number;
  lists: number;
  tabs: number;
  dropdowns: number;
  icons: number;
}

interface TableAnalysis {
  index: number;
  headers: string[];
  rows: number;
  classes: string;
  id: string | null;
}
```

## Quality Standards

### Completeness Checklist

- [ ] All main feature views captured
- [ ] Detail pages for multiple items
- [ ] Dropdowns and menus documented
- [ ] Sort variations captured (if applicable)
- [ ] Filter options documented (if applicable)
- [ ] Tables analyzed with headers
- [ ] Forms documented with fields
- [ ] Configuration pages captured
- [ ] Export options documented
- [ ] Navigation paths mapped

### Documentation Quality

- [ ] Status report generated with statistics
- [ ] README created with instructions
- [ ] Sitemap reports in Markdown + JSON
- [ ] Database schema recommendations
- [ ] API endpoint list
- [ ] Frontend component requirements
- [ ] Implementation insights
- [ ] Troubleshooting guidance

### File Organization

- [ ] Standardized directory structure
- [ ] Consistent naming conventions
- [ ] All screenshots high quality
- [ ] All DOM files complete
- [ ] All metadata files valid JSON
- [ ] Reports properly formatted
- [ ] Cross-references working

## Error Handling

### Common Issues and Solutions

**Authentication Failure:**
```javascript
if (await page.locator('.error-message').isVisible()) {
  throw new Error("Login failed. Check credentials.");
}
```

**Page Timeout:**
```javascript
try {
  await page.goto(url, { timeout: 60000 });
} catch (error) {
  console.log(`Timeout on ${url}, skipping...`);
  continue;
}
```

**Missing Elements:**
```javascript
const elements = await page.locator(selector).count();
if (elements === 0) {
  console.log(`No elements found for ${selector}`);
}
```

**Incomplete Crawl:**
```javascript
if (visitedUrls.size < expectedPages) {
  console.warn(`Only captured ${visitedUrls.size} of ${expectedPages} pages`);
}
```

## Integration with Development Workflow

### For Database Design
- Extract table structures from UI tables
- Infer columns from form fields
- Identify relationships from links
- Document constraints from validation

### For API Development
- Map CRUD operations from buttons
- Identify endpoints from network requests
- Document query parameters from filters
- Extract response structures from DOM

### For Frontend Development
- Screenshot reference for layouts
- Component inventory for design system
- Interaction patterns for UX
- Color schemes for theming

### For Testing
- User flows from navigation paths
- Test data from captured items
- Edge cases from validation rules
- Scenarios from workflow states

## Success Metrics

A successful crawl delivers:
- **Coverage**: 40-50 pages captured
- **Quality**: Clear, full-page screenshots
- **Depth**: Detail pages for multiple items
- **Analysis**: Complete metadata extraction
- **Documentation**: Comprehensive reports
- **Organization**: Well-structured output
- **Usability**: Implementation-ready artifacts

## Limitations

- Cannot capture features requiring real-time data
- May miss JavaScript-heavy interactions
- Limited to visible UI elements
- Cannot extract backend business logic
- Dependent on Procore authentication
- May timeout on slow pages
- Cannot capture dynamic content that requires specific triggers

## Best Practices

1. **Start with core flows** - Capture main list → detail → create/edit
2. **Explore systematically** - Don't randomly click, follow logical paths
3. **Document as you go** - Add notes about special interactions
4. **Verify completeness** - Check if all major features captured
5. **Test outputs** - Ensure screenshots and metadata are usable
6. **Organize immediately** - Don't leave files scattered
7. **Report clearly** - Make findings actionable for developers

## Example Invocations

```typescript
// Via Task tool
Task({
  subagent_type: "feature-crawler",
  prompt: `Crawl Procore Submittals feature
Feature: submittals
Start URL: https://us02.procore.com/562949954728542/project/submittals
Max Pages: 50

Execute complete crawl workflow and generate all documentation.`,
  description: "Crawl Submittals feature"
})

// Via Skill tool (invoked by /feature-crawl)
Skill({
  skill: "feature-crawl",
  args: "submittals https://us02.procore.com/562949954728542/project/submittals"
})
```

## Deliverables

Upon completion, this subagent delivers:

1. **Crawl Output Directory** (`crawl-{feature}/`)
   - 40-50 page directories with screenshots, DOM, metadata
   - Complete sitemap reports
   - Link graph analysis

2. **Status Report** (`{FEATURE}-CRAWL-STATUS.md`)
   - Summary statistics
   - Key pages captured
   - Structure analysis
   - Implementation roadmap

3. **README** (`README.md`)
   - Crawler documentation
   - Usage instructions
   - Troubleshooting guide

4. **User Summary**
   - Completion confirmation
   - Statistics and highlights
   - Output location
   - Next steps for implementation

---

**Agent Maintainer:** Alleato Development Team
**Last Updated:** 2026-01-10
**Version:** 1.0.0
